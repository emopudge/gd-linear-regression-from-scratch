# Линейная регрессия с градиентным спуском «с нуля»

Этот проект демонстрирует **ручную реализацию линейной регрессии** с использованием **градиентного спуска** без библиотек вроде `sklearn.linear_model`.  
Цель — показать понимание математики и алгоритмов машинного обучения «под капотом».

 **Весь код написан вручную — без Copilot, ChatGPT и автодополнения.**  
Цель — думать головой, а не копировать решения.

---

## Задача
Предсказать медианную стоимость жилья (`MEDV`) в Бостоне на основе признаков (например, среднее число комнат `RM`).

---

## Алгоритм

Используется алгоритм **множественной линейной регрессии** для описания зависимости нескольких переменных друг от друга (в нашем случае зависимость цены (у, или h(x)) от набора метрик (х_1, х_2...): площади, расположения дома и так далее).

Формула линейной регрессии:

<img width="361" height="35" alt="image" src="https://github.com/user-attachments/assets/eabd7da2-084f-4b5c-b2bc-dced29c75561" />

где 
- θ - коэффициенты регрессии для каждого признака

В матричной форме:

<img width="88" height="37" alt="image" src="https://github.com/user-attachments/assets/bd886ed8-7cca-4780-ab08-c992184e6533" />

где 
- y - целевой вектор предсказаний
- Х - матрица признаков
- θ - вектор коэффициентов



**MSE** (Mean Squared Error - среднеквадратичная ошибка, чем она меньше, тем модель лучше обучена) - одна из **функций потерь** (loss function - показывает, насколько мала разность между истинным значением и предсказанным). 

Формула MSE:

<img width="313" height="88" alt="image" src="https://github.com/user-attachments/assets/1c6f6e14-4195-4da1-89ea-a7d25c810a00" />

где 
- m - количество обучающих примеров
- x_i - вектор признаков i-го примера (строка из Х)
- y_i - истинное значение i-го примера
- h(x_i) - предсказание модели для i-го примера
- множитель 1/2 добавлен для удобства дифференцирования и сокращается при взятии производной

В матричной форме:
<img width="311" height="68" alt="image" src="https://github.com/user-attachments/assets/bb4a2128-bb35-4190-a6d5-747dc421837b" />


В коде:
```python
y_pred = X @ self.theta          # h_θ(x) = Xθ
error = y_pred - y               # (h_θ(x) - y)
gradient = (1 / m) * X.T @ error # ∇J = (1/m) * Xᵀ * (Xθ - y)
loss = (1 / (2 * m)) * np.dot(error, error)  # J(θ) = (1/(2m)) * Σ(error²) (функция потерь)
```



Обновление параметров (градиентный спуск):

<img width="327" height="127" alt="image" src="https://github.com/user-attachments/assets/c49ec908-da08-45f2-bdae-fbca486dace2" />

где alpha - скорость обучения модели (learning rate)

в коде:
```python
self.theta -= self.alpha * gradient  # θ := θ - α * ∇J
```
---

## Реализация

Класс `GDRegressor` поддерживает:
- Обучение (`fit`)
- Предсказание (`predict`)
- Масштабирование признаков (`z_scaler`)
- Расчёт метрик: RMSE, R²
- Подбор гиперпараметров (`find_optimal_params`)
- Визуализацию сходимости

---

## Результаты на Boston Housing
- **RMSE ≤ 6.45**
- **R² ≥ 0.49**
- Результаты совпадают с `sklearn.LinearRegression` (проверено в тестах)

---

## Как запустить

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/ваш-ник/gd-linear-regression-from-scratch.git
   cd gd-linear-regression-from-scratch
   ```
2. Установите зависимости
   ```bash
   pip install -r requirements.txt
   ```
3. Выполните тесты:
   ```bash
   python test_linreg.py
   ```

## Использованные технологии
Python 3.9+
NumPy — для матричных операций
Pandas — загрузка данных
Matplotlib — визуализация
Scikit-learn — только для сравнения (не в обучении!)
