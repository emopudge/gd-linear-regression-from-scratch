# Линейная регрессия с градиентным спуском «с нуля»

Этот проект демонстрирует **ручную реализацию линейной регрессии** с использованием **градиентного спуска** без библиотек вроде `sklearn.linear_model`.  
Цель — показать понимание математики и алгоритмов машинного обучения «под капотом».

 **Весь код написан вручную — без Copilot, ChatGPT и автодополнения.**  
Цель — думать головой, а не копировать решения.

---

## Задача
Предсказать медианную стоимость жилья (`MEDV`) в Бостоне на основе признаков (например, среднее число комнат `RM`).

---

## Алгоритм
### Множественная линейная регрессия

Используется алгоритм **множественной линейной регрессии** для описания зависимости нескольких переменных друг от друга (в нашем случае зависимость цены (у, или h(x)) от набора метрик (х_1, х_2...): площади, расположения дома и так далее).

Формула линейной регрессии:

<img width="361" height="35" alt="image" src="https://github.com/user-attachments/assets/eabd7da2-084f-4b5c-b2bc-dced29c75561" />

где 
- h_θ(x) — предсказанное значение цены

- θ₀ — свободный член (intercept)

- θ₁...θₙ — коэффициенты регрессии для каждого признака

- x₁...xₙ — значения признаков

В матричной форме:

<img width="232" height="66" alt="image" src="https://github.com/user-attachments/assets/f33399f0-032e-4682-a0c9-3426ed1e6bc0" />

где 
- X — матрица признаков (с добавленным столбцом единиц для θ₀)

- θ — вектор коэффициентов


### Функция потерь (MSE)

Для оценки качества обучения модели используем MSE. 

**MSE** (Mean Squared Error - среднеквадратичная ошибка, чем она меньше, тем модель лучше обучена) - одна из **функций потерь** (loss function - показывает, насколько мала разность между истинным значением и предсказанным). 

Формула MSE:

<img width="313" height="88" alt="image" src="https://github.com/user-attachments/assets/1c6f6e14-4195-4da1-89ea-a7d25c810a00" />

где 
- m - количество обучающих примеров
- x_i - вектор признаков i-го примера (строка из Х)
- y_i - истинное значение i-го примера
- h(x_i) - предсказание модели для i-го примера
- множитель 1/2 добавлен для удобства дифференцирования и сокращается при взятии производной

В матричной форме:
<img width="311" height="68" alt="image" src="https://github.com/user-attachments/assets/bb4a2128-bb35-4190-a6d5-747dc421837b" />


В коде:
```python
y_pred = X @ self.theta          # h_θ(x) = Xθ
error = y_pred - y               # (h_θ(x) - y)
loss = (1 / (2 * m)) * np.dot(error, error)  # J(θ) = (1/(2m)) * Σ(error²) (функция потерь)
```


### Градиентный спуск

<img width="485" height="470" alt="image" src="https://github.com/user-attachments/assets/7d2a2e90-ee31-4bac-b551-c23034521bcc" />

Для минимизации MSE используется градиентный спуск. На каждой итерации параметры модели обновляются (вычитается произведение градиента и шага (у нас шагом является learning rate)), стремясь к минимуму функции потерь

**Градиент** — это вектор первых частных производных, который показывает направление, в котором функция растёт быстрее всего, а также скорость изменения функции
Обновление параметров (градиентный спуск):

<img width="327" height="127" alt="image" src="https://github.com/user-attachments/assets/c49ec908-da08-45f2-bdae-fbca486dace2" />

где 

- alpha - скорость обучения модели (learning rate)
- частная производная - градиент

Градиент в матричной форме:

<img width="427" height="99" alt="image" src="https://github.com/user-attachments/assets/74582ddc-4b48-4768-9f65-40497c2e8b20" />


в коде:
```python
gradient = (1/m) * X.T @ error    # ∇J = (1/m) * Xᵀ * (Xθ - y)
self.theta -= self.alpha * gradient  # θ := θ - α * ∇J
```
---

## Реализация

Класс `GDRegressor` поддерживает:
- Обучение (`fit(X_train, y_train)`)
- Предсказание (`predict(X_test)`)
- Масштабирование признаков (`z_scaler`)
- Расчёт метрик: RMSE, R²
- Подбор гиперпараметров `alpha`, `n_iter` (`find_optimal_params()`)
- Визуализацию сходимости (`plot_loss_function()`, `plot_coeffs()`)

---

## Результаты на Boston Housing
- **RMSE ≤ 6.45**
- **R² ≥ 0.49**
- Результаты совпадают с `sklearn.LinearRegression` (проверено в тестах)

---

## Как запустить

1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/emopudge/gd-linear-regression-from-scratch.git
   cd gd-linear-regression-from-scratch
   ```
2. Установите зависимости
   ```bash
   pip install -r requirements.txt
   ```
3. Выполните тесты:
   ```bash
   python GDR_class_test.py
   ```
4. Запустите пример использования (файл `example_usage.py`) (для простоты визуализируем только линейную регрессию зависимости цены и количества комнат)

## Использованные технологии

Python 3.9+

NumPy — для матричных операций

Pandas — загрузка данных

Matplotlib — визуализация

Scikit-learn — только для сравнения (не в обучении!)
